---
layout: post
title: "Property-based testing, adversarial developers, and LLMs"
date: 2025-12-22
author: Igor Konnov
categories: pbt
tlaplus: true
math: true
shell: true
python: true
hidden: true
feed: false
---

**Author:** [Igor Konnov][]

**Date:** December 22, 2025

I present a simple example that illustrates how property-based testing (PBT) and
model checking can help us catch unexpected behaviors of LLMs when they are used
to generate code. The example is inspired by the [talk on property-based
testing][scott-pbt] by [Scott Wlaschin][]. If you find my other blog posts too
complicated, this one is for you!

## 1. Adversarial Developer

A few days ago, I watched the [talk on property-based testing][scott-pbt] by
[Scott Wlaschin][]. He started the talk by introducing a persona that he called
the **Enterprise Developer from Hell**. This is basically someone who
implements a feature to satisfy the given requirements, but they do it
creatively evil (or just stupid) and unexpected ways. I will call such a persona
an **adversarial developer** in the rest of this post.

Then, Scott[^scott-talk] showed how an adversarial developer could ruin as simple task as
adding up two numbers. For example, if we give them two tests $2+2=4$ and $10+33
= 43$, they will implement exactly those cases by case distinction. I am not
going to repeat Scott's talk. [Watch it][scott-pbt]! It's instructive and
entertaining.

Back in 2020, of course, Scott added that we are often aversarial developers
ourselves, and our peers are rarely that evil. Now, we are a few weeks away from
2026, and we have such a peer! It is called an LLM, or just AI, as the corporate
marketers prefer. LLMs are not necessarily evil, but they are definitely less
predictable than human software engineers. I am not talking about [prompt
injection][] here, which is another real issue with LLMs.

## 2. Property-Based Testing

The point of Scott's talk was to show that a few data points (typical unit
tests) are insufficient to demonstrate correctness of the implementation.
A totally valid point!

In addition to the standard unit tests, we should also write the expected
properties of our implementation. The PBT frameworks test the code by producing
input values at random. For example, have a look at [Hypothesis][]. While this
may seem to be a silly idea at first, property-based tests may uncover tricky
bugs. Moreover the input value distribution does not have to be uniform. Keep
reading to see how this helps us catch the adversarial developer.

Here are the three properties of addition that Scott used to defeat the
adversarial developer: 

 - **identity**: for any number $x$, $x + 0 = x$,
  - **commutativity**: for any numbers $x$ and $y$, $x + y = y + x$, and
  - **associativity**: for any numbers $x$, $y$, and $z$, $(x + y) + z = x + (y +
    z)$.

At this point of the talk, I was like: Wait a minute! I could continue this game
of the adversarial developer. Before continuing with the game, let's look at
where we are with respect to the code and the properties. Here is the obvious
implementation of integer addition in Python, since the language has built-in
support for unbounded integers:

{% github_embed
  https://raw.githubusercontent.com/konnov/pbt-example-summation/refs/heads/main/python/src/pbt_add/add.py
  python 4-5
 %}

Here are the property-based tests in [Hypothesis][], generated by Claude Sonnet 4.5:

{% github_embed
  https://raw.githubusercontent.com/konnov/pbt-example-summation/2164991e4d0a9891fc4919843dabe21775421bf0/python/tests/test_add.py
  python 9-25
 %}

You can find these and further examples in the [example repository][example-repo].

We run these tests with `pytest` to make sure that they all pass:

```sh
$ git clone https://github.com/konnov/pbt-example-summation.git
$ cd pbt-example-summation/python
$ poetry run pytest tests/test_add.py \
  -k "test_identity or test_commutativity or test_associativity" --verbose
...
tests/test_add.py::test_identity PASSED                                        [ 33%]
tests/test_add.py::test_commutativity PASSED                                   [ 66%]
tests/test_add.py::test_associativity PASSED                                   [100%]
```

## 3. Symbolic model checking with Apalache

I decided to go even further and write a TLA<sup>+</sup> specification, to check
the three properties with [Apalache][] and [Z3][] (only show the relevant parts):

{% github_embed
  https://raw.githubusercontent.com/konnov/pbt-example-summation/2164991e4d0a9891fc4919843dabe21775421bf0/tla-spec/Add.tla
  tlaplus 1-17,24-28,44-54,62-66
 %}

With the above specification, we define a very simple state machine that
non-deterministically picks three integers `x`, `y`, and `z` with `InitMath`.
These variables do not change in the state machine, as you can see from the
definition of `Next`. We use `x`, `y`, and `z` to define three properties of
addition: `Identity`, `Commutativity`, and `Associativity`. As you can see,
these definitions are parameterized by the operator `F`, which is `Add` for now.
Our invariant `InvMath` is simply the conjunction of the three properties.

This is how we run Apalache to check the invariant:

```sh
$ cd pbt-example-summation/tla-spec
$ docker pull ghcr.io/apalache-mc/apalache
$ docker run --rm -v `pwd`:/var/apalache ghcr.io/apalache-mc/apalache \
  check --length=0 --init=InitMath --inv=InvMath Add.tla
```

With the above command, we tell Apalache to check the invariant `InvMath`
starting from the initial state `InitMath`. The `--length=0` option tells
Apalache to unroll `Next` zero times, which is sufficient in our case, since the
state machine does not change the variables.


## 4. Playing adversarial

Ok, the code and the specification above seem to be correct. But what if our
friendly AI produced something unexpected?

### 4.1. Hallucinating addition over 32-bit unsigned integers

Since we are dealing with an adversarial developer, they could simply use a
different definition of addition. So far, we have been talking about unbounded
mathematical integers, which Python conveniently implements for us.

Now, the adversarial developer gives us this implementation:

{% github_embed
  https://raw.githubusercontent.com/konnov/pbt-example-summation/42f782ff09c6a957fc36b33d3a4b933674e9799d/python/src/pbt_add/add.py
  python 8-9
 %}

This implementation is actually not wrong. An LLM could copy it from a code base
that emulates a 32-bit CPU architecture in Python. This is a bit of a stretch,
but possible.

Let's add property-based tests for this implementation as well:

{% github_embed
  https://raw.githubusercontent.com/konnov/pbt-example-summation/42f782ff09c6a957fc36b33d3a4b933674e9799d/python/tests/test_add.py
  python 26-48
 %}

These tests also pass:

```sh
$ poetry run pytest tests/test_add.py \
  -k "test_add32_identity or test_add32_commutativity or test_add32_associativity" \
  --verbose
```

What is going on? Well, identity, commutativity, and associativity also hold for
32-bit integers with overflow semantics. **If we let AI generate not only the
implementation but also the properties, we may end up with a correct
implementation, but not the one we wanted!** This is an example of the classical
question in requirements engineering:
*do we get things right* vs. *do we get the right things*.

Just to double check that it is not the random chance, I ran Apalache on the
TLA<sup>+</sup> specification above with `Add32` instead of `Add`:

```shell
$ docker run --rm -v `pwd`:/var/apalache ghcr.io/apalache-mc/apalache \
  check --length=0 --init=Init32 --inv=Inv32 Add.tla
...
Checker reports no error up to computation length 0
Total time: 2.205 sec
```

The SMT solver Z3 confirms that identity, commutativity, and associativity also
hold for 32-bit integers with overflow semantics. This is provided that we pick
the integers from the range $[0, 2^{32})$, which we do with `Init32`.

However, **this is not what we wanted initially**. Let's catch the adversarial
developer with the PBT tests that pick unbounded non-negative integers:

{% github_embed
  https://raw.githubusercontent.com/konnov/pbt-example-summation/42f782ff09c6a957fc36b33d3a4b933674e9799d/python/tests/test_add.py
  python 75-94
 %}

This time, Hypothesis catches the issue with identity:

```sh
$ poetry run pytest tests/test_add.py  --verbose \
  -k "test_add32_unbounded_inputs_identity or test_add32_unbounded_inputs_commutativity or test_add32_unbounded_inputs_associativity"
...
a = 4294967296

    @given(st.integers(0))
    def test_add32_unbounded_inputs_identity(a):
        """Test identity property for add32: a + 0 = a."""
>       assert add32(a, 0) == a
E       assert 0 == 4294967296
E        +  where 0 = add32(4294967296, 0)
E       Falsifying example: test_add32_unbounded_inputs_identity(
E           a=4_294_967_296,
E       )
```

{% include tip.html content="We can also implement `add64` that wraps
integers modulo $2^{64}$ and the PBT tests will catch the issue with
identity almost immediately."
%}

### 4.2. Is property-based testing a magic tool?

Let's stop and think about our example. How did Hypothesis catch the issue over
an unbounded integer domain? Even it was picking the integers from the interval
$[0, 2^{64})$, the chance of picking `4294967296` by uniform random sampling is
pretty slim. Yet, Hypothesis keeps picking this number.

Well, the trick is that its input generator tries
the well-known "magic numbers" such as `0`, `1`, `-1`, `2**32`, `2**64`, etc.
In this sense, Hypothesis does not use uniform random sampling. See the
discussion on [domain and distribution][] in the Hypothesis documentation for
more details.

What if our adversarial developer hallucinated an implementation that stays
undetected by Hypothesis? This is what our next example is about.

### 4.3. Hallucinating addition over 256-bit unsigned integers

This time, the adversarial developer uses 256-bit unsigned integers with overflow
semantics:

{% github_embed
  https://raw.githubusercontent.com/konnov/pbt-example-summation/42f782ff09c6a957fc36b33d3a4b933674e9799d/python/src/pbt_add/add.py
  python 14-15
 %}

If you think that using 256-bit integers is absurd, well, the Ethereum Virtual
Machine (EVM) does exactly that. So an LLM could have adapted the above code
from an EVM-related code base.

{% github_embed
  https://raw.githubusercontent.com/konnov/pbt-example-summation/42f782ff09c6a957fc36b33d3a4b933674e9799d/python/tests/test_add.py
  python 120-142
 %}

This time, the adversarial developer gets away, all tests pass:

```shell
$ poetry run pytest tests/test_add.py --verbose -k \
  "test_add256_unbounded_inputs_identity or test_add256_unbounded_inputs_commutativity or test_add256_unbounded_inputs_associativity" 
...
tests/test_add.py::test_add256_unbounded_inputs_identity PASSED                [ 33%]
tests/test_add.py::test_add256_unbounded_inputs_commutativity PASSED           [ 66%]
tests/test_add.py::test_add256_unbounded_inputs_associativity PASSED           [100%]
```

Why? Hypothesis does not try $2^{256}$ as a magic number. I gave it the budget of
100,000 examples, so it had a chance to try multiple powers of two, but it did
not try anything above $2^{256} - 1$.

## 4.4. Catching the adversarial developer with Apalache

Apalache immediately finds the issue with identity when we run it with `Add256`:

```shell
$ docker run --rm -v `pwd`:/var/apalache ghcr.io/apalache-mc/apalache \
  check --length=0 --init=InitNat --inv=Inv256 Add.tla
...
State 0: state invariant 0 violated.
Total time: 2.272 sec
```

If we check the counterexample, we see that the solver picks quite a large
number for `x`:

```sh
$ head -n 14 _apalache-out/Add.tla/2025-12-22T15-20-31_8166638658721555415/violation.tla
---------------------------- MODULE counterexample ----------------------------
EXTENDS Add

(* Constant initialization state *)
ConstInit == TRUE

(* Initial state [_transition(0)] *)
State0 ==
  x
      = 115792089237316195423570985008687907853269984665640564039457584007913129639936
    /\ y = 0
    /\ z = 0

```

**This is not just luck and not an heuristic!** Apalache delegates solving to
the SMT solver [Z3][], which solves integer constraints. If you want to make
sure that it's not using magic numbers, go and change the modulo operator in
`Add256` to a large prime number, e.g., $2^{256} + 297$. Rerun the model
checker, and it will still find the issue with identity.

## 5. Conclusions

We all have to **learn how to write high-quality properties and understand the
boundaries of the "magic" tools**. **Even if you don't use LLMs, your peers
will.**

**How to learn writing good properties**? We can play with property-based
testing. However, **PBT tools are not reliable teachers**. By their random
nature, a PBT tool may miss a bug on one run and find it on another run. Don't
get me wrong. **Property-based testing has its value**, as many other testing
and verification techniques. However, **PBT is not a silver bullet**. It may
miss bugs, especially if the input generator does not cover the right input
space well enough.

**Shall we use interactive provers like [Lean][] and [Rocq][]**? Learning how to
prove code correct definitely helps! However, **these tools only tell us that the
proof does not go through**. It would not give us a counterexample.
State-of-the-art provers also recommend using PBT for bug finding. 

In my opinion, **model checking is the best way to learn how to write good
properties**. You can write as many properties as you like, and the model
checker will produce you counterexamples, or not. Importantly, model checkers
come with a guarantee of not having a bug in their *search scope*, if they
terminate.  See my blog post on the [value of model
checking][model-checking-value] on that.

Usually, I recommend people to start with [TLC][]. It works by state enumeration
and easy to understand. If your search scope is small, TLC is a good learning
tool. In our example, the search scope is astronomical. In this case,
[Apalache][] is there to help.

By the way, our example was so simple, that we could encode it in [Z3][]
directly via its python bindings. We could use other model checkers. If you do
that, let me know!

<!-- References -->

[^scott-talk]: I've never met Scott Wlaschin in real life, online or offline. I hope he would not mind me referring to him by his first name.
[Igor Konnov]: https://konnov.phd
[scott-pbt]: https://youtu.be/IYzDFHx6QPY
[Scott Wlaschin]: https://scottwlaschin.com/
[Lean]: https://lean-lang.org/
[Rocq]: https://rocq-prover.org/
[TLC]: https://github.com/tlaplus/tlaplus
[Apalache]: https://apalache-mc.org
[Z3]: https://github.com/Z3Prover/z3
[Hypothesis]: https://hypothesis.readthedocs.io/en/latest/
[model-checking-value]: {% link _posts/2025-04-08-value.md %}
[prompt injection]: https://genai.owasp.org/llmrisk/llm01-prompt-injection/
[example-repo]: https://github.com/konnov/pbt-example-summation
[domain and distribution]: https://hypothesis.readthedocs.io/en/latest/explanation/domain.html